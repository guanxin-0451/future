![伴鱼架构图](https://cynthia-oss.oss-cn-beijing.aliyuncs.com/1632643763404.png)
![架构图2](https://cynthia-oss.oss-cn-beijing.aliyuncs.com/1632643792243.png)


### 设计题
#### [短链接](https://blog.csdn.net/uxiAD7442KMy1X86DtM3/article/details/110152007)
#### [微博设计](https://timyang.net/architecture/feed-sharding-practice/)
    [表设计](https://blog.csdn.net/qq_38738033/article/details/107691607)
 
#### [微信红包](https://blog.csdn.net/weixin_32460203/article/details/112404110)   
#### 秒杀系统瞬时高并发
  [秒杀系统](https://mp.weixin.qq.com/s/BxdBrgl-TR6Wz_yo07-5Lg)

  - 页面静态化  
    页面提前静态化初始化好（改价格怎么办？做强制刷新，版本控制等）   
  - CDN加速  
    网页直接挂cdn上
  - 秒杀按钮  
    控制置灰
  - 读多写少  
    库存不够时立刻返回失败 库存直接查redis 不负责下单付款结算等流程，只负责锁单。
    
    
  - 缓存
    不存在的也要入缓存，支持查空 接口级别可以加
    
  - mq异步处理
  - 限流
  - 分布式锁
  
  
    qps分析：
    5000+qps 
    单机mysql 千级别qps ，目前常规都是集群部署，也不能将请求全部落到mysql
    
    架构上选择：（5000+qps本身并不高，尽量提供扩展性好，能通过横向扩展/扩容等提高抗更高并发的能力）
    1.前端请求资源：动态数据读缓存，静态走cdn
    2. 秒杀链接：动态链接
    2.nginx 负载均衡
    3.api gateway  做限流，熔断 
    4. service 做本地缓存+redis缓存（设计时注意缓存击穿，雪崩），读库加锁。
    5. redis 集群: 操作库存 （多步操作使用lua脚本实现原子性操作），控制并发问题。
    6. MQ+db: 发mq，起消费者消费mq操作db库存扣减生成订单（削峰填谷，注意消息失败&重复消费的问题）
    
    
    
#### 抽样平台
  - 现状： 各业务各自维护组件： redis/mysql等各自部署，没有公用的消息队列，配置中心）—apollo—等。
    需要考虑成本
  - 染色服务
    - 搜索
        - 老版本 跟nginx上线，作为配置，影响面广。
        - 新版本 服务化，作为配置通过redis上线。
        
    - 推荐
      - 配置文件热更新
      
  - 上线平台：后台 && 调度服务（配送服务）
  - 流量上线
    - 推荐
      - 没用redis : 成本及推荐为接手的业务。
      - 通过文件配送，配送正交表&&实验配置到线上实例。
    - 搜索
      - redis 配送正交表 && 实验配置（过滤配置）
      - 调度服务: 写需要上线的redis集群。（类似代码上线到服务集群的某些集群（华北a,b....））
      - 一个集群写1个实例。 
      - 拆正交表 取模，按行。

  - 策略上线: 支持多模块上线，
    - 各模块标准化接入: 模块代码sdk：读本地固定路径（抽样配送的路径），启动，n分钟读取一次本地化配置。
    - 请求上游接参数，带下来sample_id，有trace_id（qid）追踪，触发对应变量，获取变量对应的值，触发对应策略。
    - 上线通过各模块的上线单，走代码上线流程，将文件推送到线上机器。
  
  - 文件配送：
    - 后台生产到本地，生成本地文件，触发mis（文件云存储，版本管理），通过afs/ftp拉取本地文件，上传到mis。（类似代码上线流程中的代码和到了代码库）
    - 发起上线单，拉取对应mis的某版本文件，配送到线上实例。
    
  
  - 工作内容：
    - 服务融合，减少维护成本。
            1. 迁移推荐的配置后台到新服务。
            迁移现状：
              - 染色服务为统一（即线上流量获取实验统一）
              - 配送方式不同，配送配置文件不同
            迁移方案：
              1. 上下游接口串联梳理： 提供给业务方的查询接口（原接口做merge转发）， 所有节点的下游推送
              3. 上线流程：
                 1. 旧服务封禁上线，新服务开启上线。
                 2. 旧服务接口做merge 可合并查询新服务接口。
                 3. 旧服务可正常暂停/下线实验，新服务监听旧服务动作，merge旧服务产出文件。
                 4. 旧服务在线实验存量消化，同时业务方开始接新接口。
                 5. 消化完成，下线。
            困难点： 无文档，需要统计日志等，梳理上下游流程，确保无遗漏。
            
    稳定性优化: 
        2. 解决幂等性问题。
          - 1.创建时会产生多条数据。 
          - 2.多次调用不能保证行为一致
            - 重新触发导致推下游接口异常： 重复
          - 解决方案：
            - 前端的button统一加了无返回禁止重复点击处理。 通过promise。
            - 有明确外键限制的： 库表对应加duplicate限制，例如user创建。
            - 主要流程的post接口，如实验，创建快照，创建流量，点击上线等，增加token验证机制。
              前端获取（过期时间默认2分钟，前端加带1min刷新机制），获取时机可能是页面加载，二级页面弹出等，不能统一加，只能分情况加。
              后端对应post接口统一加了处理（通过闭包的方式），对处理token加锁，验证token，删除token。删除锁。
        3. 解决流量分配死锁问题：
           同流量层分配需要顺序处理，不能并行。需要加锁。
           一个实验可能申请多层：对应普通浏览器，手百，手百lite等。
           原方案：goroutine协程处理。 处理可能成环，死锁。
           解决: 申请层时按排序申请，加锁顺序加锁，处理还用协程。遇到不能加锁的，自动重试。
        4. go服务配送改造：
           1.处理回调的方式：
             - 原：每个操作阶段都起1个goroutine执行：如果需要回调，则goroutine执行死循环等待。 等待由接口触发的channel 接受不了channel来处理。
             - 新： 起1个统一的消费者，原goroutine运行到等待后，则记录上下文到redis。退出。  
                   1个channel作为消费者，统一接收请求信号，从redis查询，拉取对应上下文，进行处理。
           2. 统一的三方接口重试机制。
           3. 日志升级： 统一ctx+trace方式打标准化日志。
           4. 分配流量，上线流程的 分布式一致性保证：
              所有操作前，增加查询业务的数据，确保是否操作过，或者已经在线。
             
             
             - 好处：减少开销，为分布式部署做基础。